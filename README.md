### DescriÃ§Ã£o
Este projeto cria um chatbot local em linha de comando utilizando o modelo gemma3:1b via Ollama. Ele serve como uma base sÃ³lida para a criaÃ§Ã£o de interfaces conversacionais privadas, rÃ¡pidas e que nÃ£o dependem da internet ou de APIs de terceiros.

Por ser modular, o chatbot pode ser expandido com:
- AutenticaÃ§Ã£o de usuÃ¡rios
- HistÃ³rico de conversas
- Interface web com Flask ou Streamlit
- IntegraÃ§Ã£o com bancos de dados
- Suporte a mÃºltiplos modelos
- Ã‰ ideal para quem deseja estudar o funcionamento de LLMs locais, criar protÃ³tipos rÃ¡pidos, ou garantir mais controle e privacidade na execuÃ§Ã£o de modelos de linguagem.

### Tecnologias
ðŸ§  Ollama â€“ para execuÃ§Ã£o local do modelo de AI LLM gemma3:1b (um SLM poderoso)
ðŸ Python 3.8+
ðŸ“¦ requests â€“ biblioteca Python para comunicaÃ§Ã£o com o servidor local do Ollama

### Estrutura
```
ollama-chatbot-cli/
â”œâ”€â”€ chatbot.py
â”œâ”€â”€ config.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env
```

### InstalaÃ§Ã£o
```bash
git clone https://github.com/4kumon/ollama-chatbot-cli.git
cd ollama-chatbot-cli
pip install -r requirements.txt
```

[Clique aqui para baixar](https://ollama.com/download) e instalar o Ollama para seu sistema operacional

Carregue o modelo ```gemma3:1b```
```ollama run gemma3:1b```

Isso iniciarÃ¡ um servidor local em ```http://localhost:11434```.

### Uso
Com o Ollama rodando em segundo plano:
```bash
python main.py
```
VocÃª poderÃ¡ conversar com o modelo diretamente pelo terminal.

### Diagrama UML
```mermaid
sequenceDiagram
    participant U as UsuÃ¡rio
    participant CLI as Chatbot (Terminal)
    participant Ollama as Modelo Local (gemma3:1b)

    U->>CLI: Digita uma pergunta
    CLI->>Ollama: Envia prompt para geraÃ§Ã£o
    Ollama-->>CLI: Retorna resposta
    CLI-->>U: Mostra resposta

```
